# network architecture
model_conf:
    is_json_cmvn: true
    cmvn_file: ../../pre-trained/gigaspeech_20210728_u2pp_conformer_exp/global_cmvn

    d_model: 512  # dimension of attention
    attention_heads: 8
    linear_units: 2048  # the number of units of position-wise feed forward
    dropout_rate: 0.1
    length_normalized_loss: false
    # encoder related
    input_layer: 'conv2d6'
    pos_enc_layer_type: 'rel_pos' # rel_pos is for conformer
    encoder_num_blocks: 12   # the number of encoder blocks

    # conformer config
    macaron_style: true # true is for conformer
    use_cnn_module: true # true is for conformer
    cnn_module_kernel: 31
    casual: true
    
    # adapter config
    encoder_use_adapter: false
    decoder_use_adapter: false
    down_size: 64
    scalar: 0.1 # -1 means learnable, 0.1-1

    #AED config
    ctc_weight: 0.3
    lsm_weight: 0.1
    decoder_num_blocks: 3

    # bidecoder config
    r_decoder_num_blocks: 3
    reverse_weight: 0.3


# feature extraction
collate_conf:
    normalization: false
    # spec level config
    feature_extraction_conf:
        resample_rate: 16000
        # online speed perturb conf
        speed_perturb_rate: 0 # must be 0 if kaldi
        speeds: [0.9, 1.1, 0.1] # [start, end, interval]
        wav_dither: 0.0
        mel_bins: 80
    feature_dither: 0.0 # add dither [-feature_dither,feature_dither] on fbank feature
    spec_sub: false
    spec_sub_conf:
        num_t_sub: 3
        max_t: 30
    spec_aug: true
    spec_aug_conf:
        num_t_mask: 3
        num_f_mask: 2
        max_t: 50
        max_f: 10

# dataset related
dataset_conf:
    # offline speed perturb conf
    speed_perturb: false
    speeds: [0.9, 1.1, 0.1]
    max_length: 2000
    min_length: 10
    batch_type: 'dynamic' # dynamic or static, shuffle
    max_frames_in_batch: 10000 # if dynamic
    batch_size: 12 # if staticd
    sort: true
    raw_wav: true # must be false if kaldi else true

grad_clip: 5
accum_grad: 1
max_epoch: 100
log_interval: 100

optim: adam
optim_conf:
    lr: 0.001
scheduler: warmuplr     # pytorch v1.1.0+ required
warmup_epoch: 10




